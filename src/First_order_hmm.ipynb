{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Definition de l'HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from numpy import array, ones, zeros, multiply\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "UNK = \"<unk>\"  # token to map all out-of-vocabulary words (OOVs)\n",
    "UNKid = 0      # index for UNK\n",
    "epsilon=1e-100\n",
    "\n",
    "class HMM:\n",
    "        def __init__(self, state_list, observation_list,\n",
    "                 transition_proba = None,\n",
    "                 observation_proba = None,\n",
    "                 initial_state_proba = None, smoothing_obs = 0.01):\n",
    "            \"\"\"\n",
    "            Builds a Hidden Markov Model\n",
    "            * state_list is the list of state symbols [q_0...q_(N-1)]\n",
    "            * observation_list is the list of observation symbols [v_0...v_(M-1)]\n",
    "            * transition_proba is the transition probability matrix\n",
    "                [a_ij] a_ij = Pr(Y_(t+1)=q_i|Y_t=q_j)\n",
    "            * observation_proba is the observation probablility matrix\n",
    "                [b_ki] b_ki = Pr(X_t=v_k|Y_t=q_i)\n",
    "            * initial_state_proba is the initial state distribution\n",
    "                [pi_i] pi_i = Pr(Y_0=q_i)\"\"\"\n",
    "            print \"HMM creating with: \"\n",
    "            self.N = len(state_list)       # number of states\n",
    "            self.M = len(observation_list) # number of possible emissions\n",
    "            UNKid = self.M+1;                   # should not correspond to zeroth index\n",
    "            print str(self.N)+\" states\"\n",
    "            print str(self.M)+\" observations\"\n",
    "            self.omega_Y = state_list\n",
    "            self.omega_X = observation_list\n",
    "            if transition_proba is None:\n",
    "                self.transition_proba = zeros( (self.N, self.N), float) \n",
    "            else:\n",
    "                self.transition_proba=transition_proba\n",
    "            if observation_proba is None:\n",
    "                self.observation_proba = zeros( (self.M+1, self.N), float) \n",
    "            else:\n",
    "                self.observation_proba=observation_proba\n",
    "            if initial_state_proba is None:\n",
    "                self.initial_state_proba = zeros( (self.N,), float ) \n",
    "            else:\n",
    "                self.initial_state_proba=initial_state_proba\n",
    "            self.make_indexes() # build indexes, i.e the mapping between token and int\n",
    "            self.smoothing_obs = smoothing_obs \n",
    "            \n",
    "        def make_indexes(self):\n",
    "            \"\"\"Creates the reverse table that maps states/observations names\n",
    "            to their index in the probabilities array\"\"\"\n",
    "            self.Y_index = {}\n",
    "            for i in range(self.N):\n",
    "                self.Y_index[self.omega_Y[i]] = i\n",
    "            self.X_index = {}\n",
    "            for i in range(self.M):\n",
    "                self.X_index[self.omega_X[i]] = i\n",
    "      \n",
    "        def get_observationIndices( self, observations ):\n",
    "            \"\"\"return observation indices, i.e \n",
    "            return [self.O_index[o] for o in observations]\n",
    "            and deals with OOVs\n",
    "            \"\"\"\n",
    "            indices = zeros( len(observations), int )\n",
    "            k = 0\n",
    "            for o in observations:\n",
    "                if o in self.X_index:\n",
    "                    indices[k] = self.X_index[o]\n",
    "                else:\n",
    "                    indices[k] = UNKid\n",
    "                k += 1\n",
    "            return indices\n",
    "\n",
    "    \n",
    "        def data2indices(self, sent): \n",
    "            \"\"\"From one (letter,correction) pair \n",
    "            - extract the letter and correction \n",
    "            - returns two list of indices, one for each\n",
    "            -> (letterid, correctionid)\n",
    "            \"\"\"\n",
    "            letterids = list()\n",
    "            correctionids  = list()\n",
    "            for couple in sent:\n",
    "                letter = couple[0]\n",
    "                correction = couple[1]\n",
    "                if letter in self.X_index:\n",
    "                    letterids.append(self.X_index[letter])\n",
    "                else:\n",
    "                    letterids.append(UNKid)\n",
    "                correctionids.append(self.Y_index[correction])\n",
    "            return letterdids, correctionids\n",
    "            \n",
    "        def observation_estimation(self, pair_counts):\n",
    "            \"\"\" Build the observation distribution: \n",
    "                observation_proba is the observation probablility matrix\n",
    "                    [b_ki],  b_ki = Pr(X_t=v_k|Y_t=q_i)\"\"\"\n",
    "            # fill with counts\n",
    "            for pair in pair_counts:\n",
    "                wrd=pair[0]\n",
    "                tag=pair[1]\n",
    "                cpt=pair_counts[pair]\n",
    "                k = self.M # for <unk>\n",
    "                if wrd in self.X_index: \n",
    "                    k=self.X_index[wrd]\n",
    "                i=self.Y_index[tag]\n",
    "                self.observation_proba[k,i]=cpt\n",
    "            # normalize\n",
    "            self.observation_proba=self.observation_proba+self.smoothing_obs\n",
    "            self.observation_proba=self.observation_proba/self.observation_proba.sum(axis=0).reshape(1,self.N)\n",
    "            \n",
    "        \n",
    "        def transition_estimation(self, trans_counts):\n",
    "            \"\"\" Build the transition distribution: \n",
    "                transition_proba is the transition matrix with : \n",
    "                [a_ij] a[i,j] = Pr(Y_(t+1)=q_i|Y_t=q_j)\n",
    "            \"\"\"\n",
    "            # fill with counts\n",
    "            for pair in trans_counts:\n",
    "                i=self.Y_index[pair[1]]\n",
    "                j=self.Y_index[pair[0]]\n",
    "                self.transition_proba[i,j]=trans_counts[pair]\n",
    "            # normalize\n",
    "            self.transition_proba=self.transition_proba/self.transition_proba.sum(axis=0).reshape(1,self.N)\n",
    "        \n",
    "        def init_estimation(self, init_counts):\n",
    "            \"\"\"Build the init. distribution\"\"\"\n",
    "            # fill with counts\n",
    "            for tag in init_counts:\n",
    "                i=self.Y_index[tag]\n",
    "                self.initial_state_proba[i]=init_counts[tag]\n",
    "            # normalize\n",
    "            self.initial_state_proba=self.initial_state_proba/sum(self.initial_state_proba)\n",
    "             \n",
    "        \n",
    "        def supervised_training(self, pair_counts, trans_counts,init_counts):\n",
    "            \"\"\" Train the HMM's parameters. This function wraps everything\"\"\"\n",
    "            self.observation_estimation(pair_counts)\n",
    "            self.transition_estimation(trans_counts)\n",
    "            self.init_estimation(init_counts)\n",
    "            \n",
    "        def forward(self, obs): \n",
    "            \"\"\" Compute the forward variables for observation obs\"\"\"\n",
    "            leng=len(obs)\n",
    "            alpha = zeros((leng,self.N),float)            \n",
    "            for y in range(self.N):\n",
    "                alpha[0][y] = self.initial_state_proba[y] * self.observation_proba[self.X_index[obs[0]]][y]\n",
    "            for t in range(1, leng):  \n",
    "                k=self.X_index[obs[t]]\n",
    "                for y in range(self.N):                    \n",
    "                    alpha[t][y] = sum((alpha[t-1][y0] * self.transition_proba[y0][y] * self.observation_proba[k][y]) for y0 in range(self.N))\n",
    "            \n",
    "            return alpha\n",
    "        \n",
    "        def backward(self,obs):\n",
    "            \"\"\" Compute the backward variables for observation obs\"\"\"\n",
    "            beta=zeros((len(obs),self.N),float)\n",
    "            leng = len(obs)\n",
    "            for y in range(self.N):\n",
    "                beta[leng-1][y] = 1 \n",
    "            for t in reversed(range(leng-1)):\n",
    "                k=self.X_index[obs[t]]\n",
    "                for y in range(self.N):\n",
    "                    beta[t][y] = sum((beta[t+1][y1] * self.transition_proba[y][y1] * self.observation_proba[k][y1]) for y1 in range(self.N))\n",
    "\n",
    "            return beta\n",
    "        \n",
    "        def forward_backward(self,obs):\n",
    "            \"\"\" Compute alpha, beta and gamma matrices for observation obs\"\"\"\n",
    "            m = len(obs) #length of observation\n",
    "            n = self.N #number of states\n",
    "\n",
    "            alpha = self.forward(obs)\n",
    "            beta = self.backward(obs)\n",
    "            gamma = zeros((m, n), float)\n",
    "            for t in xrange(m):\n",
    "                gamma[t, :] = [alpha[t, i] * beta[t, i] for i in xrange(n)]\n",
    "                gamma[t, :] /= sum(gamma[t, :])\n",
    "\n",
    "            return alpha, beta, gamma\n",
    "        \n",
    "        def baum_welch(self,list_obs,maxiter=10,tresh=0.01):    \n",
    "            \"\"\"Baumd and welch algorithm for a list of observation sequences \"\"\"\n",
    "            save_trans=self.transition_proba\n",
    "            save_obs=self.observation_proba \n",
    "            \n",
    "             #Initialize parameters\n",
    "            self.initial_state_proba.fill(1/float(self.N)) #uniform\n",
    "            self.transition_proba.fill(1/float(self.N)) #uniform\n",
    "            \n",
    "            for i in range(self.N):\n",
    "                for j in range(self.M):\n",
    "                    if self.omega_Y[i]==self.omega_X[j]:\n",
    "                        self.observation_proba[j][i]=0.8 #more probable for same character\n",
    "                    else:\n",
    "                        self.observation_proba[j][i]=0.2/float(self.M-1) #unifrm for the rest\n",
    "\n",
    "            m = self.N\n",
    "            #Estimation step: compute alpha, beta gamma and xsi \n",
    "            for i in range(maxiter):\n",
    "                gammas=[] \n",
    "                gammas0=[]\n",
    "                xsis=[]\n",
    "                for obs in list_obs:\n",
    "                    tmp=[]\n",
    "                    alpha, beta, gamma=self.forward_backward(obs)\n",
    "                    gammas.append(gamma)\n",
    "                    gammas0.append(gamma[0])\n",
    "                    n = len(obs)\n",
    "\n",
    "                    xsi = np.zeros((n, m, m), float)\n",
    "                    for t in xrange(n - 1):\n",
    "                        k=self.X_index[obs[t + 1]]\n",
    "                        for i in xrange(m):\n",
    "                            for j in xrange(m):\n",
    "                                xsi[t, i, j] =alpha[t, i] * self.transition_proba[i, j] * beta[t + 1, j] * self.observation_proba[k,j]\n",
    "                                xsi[t] /= np.sum(xsi[t])\n",
    "                    xsis.append(xsi)    \n",
    "                    \n",
    "                #update parameters\n",
    "\n",
    "                self.initial_state_proba= np.sum(gammas0,axis=0)/len(list_obs)\n",
    "\n",
    "                for i in xrange(m):\n",
    "                    for j in xrange(m):\n",
    "                        self.transition_proba[i, j] = np.sum([np.sum(xsi[:, i, j]) for xsi in xsis],axis=0)\n",
    "                        self.transition_proba[i, j]  /= np.sum([np.sum(gamma[:, i]) for gamma in gammas],axis=0)\n",
    "\n",
    "\n",
    "                for i in xrange(m):\n",
    "                    for j in xrange(self.M):\n",
    "                        self.observation_proba[j,i] =0\n",
    "                        for l in range(len(gammas)):\n",
    "                            gamma=gammas[l]\n",
    "                            obs=list_obs[l]\n",
    "                            obsidx = [self.X_index[o] for o in obs]\n",
    "                            self.observation_proba[j,i] += np.sum(gamma[np.array(obsidx) == j, i]) \n",
    "\n",
    "                        self.observation_proba[j,i] /= np.sum(([np.sum(gamma[:, i]) for gamma in gammas]),axis=0)\n",
    "\n",
    "                diff_trans = np.max(save_trans- self.transition_proba)\n",
    "                diff_obs = np.max(save_obs-self.observation_proba)\n",
    "                \n",
    "                #convergence criteria\n",
    "                if diff_trans < tresh and diff_obs < tresh:\n",
    "                    break\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compter les mots et les tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_counts(corpus):\n",
    "    \"\"\" \n",
    "    Build different count tables to train a HMM. Each count table is a dictionnary. \n",
    "    Returns: \n",
    "    * c_words: word counts\n",
    "    * c_tags: tag counts\n",
    "    * c_pairs: count of pairs (word,tag)\n",
    "    * c_transitions: count of tag bigram \n",
    "    * c_inits: count of tag found in the first position\n",
    "    \"\"\"\n",
    "    c_words = dict()\n",
    "    c_tags = dict()\n",
    "    c_pairs= dict()\n",
    "    c_transitions = dict()\n",
    "    c_inits = dict()\n",
    "    for sent in corpus:\n",
    "        # we use i because of the transition counts\n",
    "        for i in range(len(sent)):\n",
    "            couple=sent[i]\n",
    "            wrd = couple[0]\n",
    "            tag = couple[1]\n",
    "            # word counts\n",
    "            if wrd in c_words:\n",
    "                c_words[wrd]=c_words[wrd]+1\n",
    "            else:\n",
    "                c_words[wrd]=1\n",
    "            # tag counts\n",
    "            if tag in c_tags:\n",
    "                c_tags[tag]=c_tags[tag]+1\n",
    "            else:\n",
    "                c_tags[tag]=1\n",
    "            # observation counts\n",
    "            if couple in c_pairs:\n",
    "                c_pairs[couple]=c_pairs[couple]+1\n",
    "            else:\n",
    "                c_pairs[couple]=1\n",
    "            # i >  0 -> transition counts\n",
    "            if i > 0:\n",
    "                trans = (sent[i-1][1],tag)\n",
    "                if trans in c_transitions:\n",
    "                    c_transitions[trans]=c_transitions[trans]+1\n",
    "                else:\n",
    "                    c_transitions[trans]=1\n",
    "            # i == 0 -> counts for initial states\n",
    "            else:\n",
    "                if tag in c_inits:\n",
    "                    c_inits[tag]=c_inits[tag]+1\n",
    "                else:\n",
    "                    c_inits[tag]=1\n",
    "                    \n",
    "    return c_words,c_tags,c_pairs, c_transitions, c_inits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# les donnÃ©es\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "train=pickle.load( open( \"../data/train10.pkl\", \"rb\" ))\n",
    "c_words,c_tags,c_pairs, c_transitions, c_inits = make_counts(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrÃ©ation du HMM et APPRENTISAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM creating with: \n",
      "26 states\n",
      "26 observations\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(state_list=c_tags.keys(), observation_list=c_words.keys(),\n",
    "                 transition_proba = None,\n",
    "                 observation_proba = None,\n",
    "                 initial_state_proba = None,\n",
    "                 smoothing_obs = 0.001)\n",
    "hmm.supervised_training(c_pairs,c_transitions,c_inits)\n",
    "#del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_states(Xd,Pi,A,B,hmm):\n",
    "\t# 'allq' stands for current estimated hidden_states\n",
    "\n",
    "\tprobs=[]\n",
    "\tstates=[]\t#new hidden-states given observations Xc\n",
    "\tfor x in Xd:\n",
    "\t\tseq = []\n",
    "        \tfor letter in x:\n",
    "            \t\tseq.append(hmm.X_index[letter[0]])\n",
    "\t\tstate,prob = viterbi(seq,Pi,A,B)\t#finding states and most likely path given these models(i.e fixed Pi,A,B)\n",
    "\t\tstates.append(state)\n",
    "\t\tprobs.append(prob)\n",
    "\n",
    "\treturn probs,states\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def viterbi(x,Pi,A,B):\n",
    "\n",
    "\t#Initialization\n",
    "\tT = len(x)\n",
    "\tN = len(Pi) # no. of states\n",
    "\tdelta_0 = []\n",
    "\tfor i in range(N):\n",
    "\t\tdelta_0.append ( np.log(Pi[i]) + np.log(B[i,x[0]]) )\n",
    "\tphi_0 = np.ones(Pi.shape)*-1\n",
    "\n",
    "\t#Recursion\n",
    "\tPhi = []\n",
    "\tprevious_delta = delta_0\n",
    "\tfor i in range(1,T):\n",
    "\n",
    "\t\tdelta_t = []\n",
    "\t\tfor j in range(N):\n",
    "\t\t\tdelta_t.append( max( previous_delta+np.log(A[:,j]) ) + np.log(B[j,x[i]]) )\n",
    "\n",
    "\t\tphi_t = []\n",
    "\t\tfor j in range(N):\n",
    "\t\t\tphi_tj = np.argmax( previous_delta+np.log(A[:,j]) )\n",
    "\t\t\tphi_t.append( phi_tj )\n",
    "\n",
    "\t\tPhi.append(phi_t)\n",
    "\t\tprevious_delta = delta_t\n",
    "\n",
    "\t#Termination\n",
    "\tS_star = max(previous_delta) #probablity of given observations\n",
    "\n",
    "\t#backward tracing of the states\n",
    "\tstates_in_inverse_order = []\n",
    "\tfinal_state = np.argmax(previous_delta)\n",
    "\tstates_in_inverse_order.append(final_state)\n",
    "#\tpdb.set_trace()\n",
    "\tfor i in range(T-2,-1,-1):\n",
    "#\t\tpdb.set_trace()\n",
    "\t\tfinal_state = Phi[i][final_state]\t\t\t\t\n",
    "\t\tstates_in_inverse_order.append(final_state)\n",
    "\treturn states_in_inverse_order,S_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Test apprentissage supervisÃ©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Pi = hmm.initial_state_proba\n",
    "A = hmm.transition_proba\n",
    "B = hmm.observation_proba\n",
    "A = A.T  #code assumes that row-wise probabilities sum to 1\n",
    "B = B.T\n",
    "# Data is assumed to comes in chains of observation, so recovering each chain of observation\n",
    "Xd=[]\n",
    "allq=[] #denotes all underlying hidden states\n",
    "test=pickle.load( open( \"../data/test20.pkl\", \"rb\" ))\n",
    "for sent in test:\n",
    "    data = np.asarray(sent)\n",
    "    obs,states = np.hsplit(data,2)\n",
    "    Xd.append(obs)\n",
    "    allq.append(states)\n",
    "del test\n",
    "Xd = np.array(Xd)\n",
    "allq = np.array(allq)   #These are the true lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_error(corrections,true_vals):\n",
    "    \"\"\"Compares the corrections and true_vals\"\"\"\n",
    "    error=0\n",
    "    total=0\n",
    "    for f, b in zip(corrections, true_vals):\n",
    "        if cmp(f,b)!=0:\n",
    "            for i in range(len(f)):\n",
    "                if f[i]!=b[i]:\n",
    "                    error+=1\n",
    "        total+=len(f)\n",
    "\n",
    "    return float(error)/float(total)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nawel/.local/lib/python2.7/site-packages/ipykernel/__main__.py:8: RuntimeWarning: divide by zero encountered in log\n",
      "/home/nawel/.local/lib/python2.7/site-packages/ipykernel/__main__.py:18: RuntimeWarning: divide by zero encountered in log\n",
      "/home/nawel/.local/lib/python2.7/site-packages/ipykernel/__main__.py:22: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "probs,allq_est =  find_states(Xd,Pi,A,B,hmm)    # Running the Viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Converting the true letters into their corresponding-index\n",
    "truevals=[]\n",
    "for trueval in allq:\n",
    "    val=[]\n",
    "    for l in trueval:\n",
    "        val.append(hmm.Y_index[l[0]])\n",
    "    truevals.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inversing the states-retrived from Viterbi (as they are recovered in inverse order)\n",
    "corrections=[]\n",
    "for q in allq_est:\n",
    "    q.reverse()\n",
    "    corrections.append(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13234677371\n"
     ]
    }
   ],
   "source": [
    "print compute_error(corrections,truevals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing baum and welch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "train=pickle.load( open( \"../data/train20.pkl\", \"rb\" )) #loading train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_obs=[] #building the unlabeled dataset\n",
    "for sent in train:\n",
    "    data = np.asarray(sent)\n",
    "    obs,states = np.hsplit(data,2)\n",
    "    tmp=[]\n",
    "    for i in range (len(obs)):\n",
    "        tmp.append(obs[i][0])\n",
    "    list_obs.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hmm = HMM(state_list=c_tags.keys(), observation_list=c_words.keys(),\n",
    "                 transition_proba = None,\n",
    "                 observation_proba = None,\n",
    "                 initial_state_proba = None,\n",
    "                 smoothing_obs = 0.001)\n",
    "hmm. baum_welch(list_obs, maxiter=2) #unsupervised training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Pi = hmm.initial_state_proba\n",
    "A = hmm.transition_proba\n",
    "B = hmm.observation_proba\n",
    "A = A.T  #code assumes that row-wise probabilities sum to 1\n",
    "B = B.T\n",
    "# Data is assumed to comes in chains of observation, so recovering each chain of observation\n",
    "Xd=[]\n",
    "allq=[] #denotes all underlying hidden states\n",
    "test=pickle.load( open( \"../data/test20.pkl\", \"rb\" ))\n",
    "for sent in test:\n",
    "    data = np.asarray(sent)\n",
    "    obs,states = np.hsplit(data,2)\n",
    "    Xd.append(obs)\n",
    "    allq.append(states)\n",
    "del test\n",
    "Xd = np.array(Xd)\n",
    "allq = np.array(allq)   #These are the true lables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs,allq_est =  find_states(Xd,Pi,A,B,hmm)    # Running the Viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting the true letters into their corresponding-index\n",
    "truevals=[]\n",
    "for trueval in allq:\n",
    "    val=[]\n",
    "    for l in trueval:\n",
    "        val.append(hmm.Y_index[l[0]])\n",
    "    truevals.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inversing the states-retrived from Viterbi (as they are recovered in inverse order)\n",
    "corrections=[]\n",
    "for q in allq_est:\n",
    "    q.reverse()\n",
    "    corrections.append(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print compute_error(corrections,truevals)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
